#!/usr/bin/env python3
"""
SSH Behaviors Test Runner

Systematically evaluates all behaviors from the SSH Behaviors Taxonomy CSV
using Bloom's pipeline with:
- Rich progress tracking with ETA and stage indicators
- State persistence for resumability
- Turn randomization (4-8 turns per behavior)
- Gemini 3 models via LiteLLM proxy
- Timestamped results to preserve all runs
- Control file monitoring for pause/stop

Usage:
    python scripts/run_ssh_behaviors.py [--resume] [--behaviors N] [--turns TURNS]

Options:
    --resume        Resume from last saved state
    --behaviors N   Limit to first N behaviors (for testing)
    --turns TURNS   Comma-separated turn counts (default: 4,5,6,7,8)
    --simple        Use simple progress display (no rich)
    --dry-run       Show what would be run without executing
    --selected      Comma-separated list of behavior slugs to run
    --run-id        Custom run ID (default: timestamp)
"""

import argparse
import csv
import json
import os
import shutil
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Optional

# Add src and scripts to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
sys.path.insert(0, str(Path(__file__).parent))

import yaml

from progress_display import create_progress_display, ProgressDisplay


# =============================================================================
# CONTROL FILE MONITORING
# =============================================================================

def load_control_file(results_dir: Path) -> dict:
    """Load the control file to check for pause/stop signals."""
    control_file = results_dir / "run_control.json"
    if control_file.exists():
        try:
            with open(control_file, "r") as f:
                return json.load(f)
        except Exception:
            pass
    return {"status": "running", "command": None}


def check_for_pause_or_stop(results_dir: Path) -> tuple[bool, bool]:
    """
    Check control file for pause/stop signals.
    Returns: (should_pause, should_stop)
    """
    control = load_control_file(results_dir)
    command = control.get("command")
    status = control.get("status")
    
    should_stop = command == "stop" or status == "stopping"
    should_pause = command == "pause" or status == "paused"
    
    return should_pause, should_stop


def clear_control_command(results_dir: Path) -> None:
    """Clear the command in the control file after processing."""
    control_file = results_dir / "run_control.json"
    if control_file.exists():
        try:
            with open(control_file, "r") as f:
                control = json.load(f)
            control["command"] = None
            control["status"] = "running"
            with open(control_file, "w") as f:
                json.dump(control, f, indent=2)
        except Exception:
            pass


def wait_for_resume(results_dir: Path, progress: ProgressDisplay) -> bool:
    """
    Wait for resume signal or stop signal.
    Returns True if should continue, False if should stop.
    """
    progress.set_stage("paused")
    print("\nâ¸ï¸  Run paused. Waiting for resume signal...", flush=True)
    
    while True:
        time.sleep(2)  # Check every 2 seconds
        control = load_control_file(results_dir)
        
        if control.get("command") == "stop" or control.get("status") == "stopping":
            return False
        
        if control.get("command") == "resume" or control.get("status") == "running":
            print("â–¶ï¸  Resuming run...", flush=True)
            clear_control_command(results_dir)  # Clear the resume command
            return True


def generate_run_id() -> str:
    """Generate a unique run ID based on timestamp."""
    return datetime.now().strftime("%Y%m%d_%H%M%S")


# =============================================================================
# STATE MANAGEMENT
# =============================================================================

class StateManager:
    """Manages persistent state for resumable test runs."""
    
    def __init__(self, state_path: Path):
        self.state_path = state_path
        self.state = self._load_state()
    
    def _load_state(self) -> dict:
        """Load state from file or create new state."""
        if self.state_path.exists():
            with open(self.state_path, "r") as f:
                return json.load(f)
        return self._create_initial_state()
    
    def _create_initial_state(self) -> dict:
        """Create initial state structure."""
        return {
            "started_at": datetime.now().isoformat(),
            "last_updated": datetime.now().isoformat(),
            "total_behaviors": 0,
            "turn_counts": [],
            "completed": {},
            "current": None,
            "failed": [],
            "config": {},
            "stage_timings": {
                "understanding": [],
                "ideation": [],
                "rollout": [],
                "judgment": [],
            },
        }
    
    def save(self) -> None:
        """Save current state to file."""
        self.state["last_updated"] = datetime.now().isoformat()
        self.state_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.state_path, "w") as f:
            json.dump(self.state, f, indent=2)
    
    def set_config(self, total_behaviors: int, turn_counts: list[int]) -> None:
        """Set configuration for this run."""
        self.state["total_behaviors"] = total_behaviors
        self.state["turn_counts"] = turn_counts
        self.state["config"]["ideation_model"] = "gemini-3-pro-preview"
        self.state["config"]["target_model"] = "gemini-3-flash-preview"
        self.save()
    
    def is_completed(self, behavior_name: str, turn_count: int) -> bool:
        """Check if a specific behavior/turn combination is completed."""
        completed = self.state["completed"].get(behavior_name, [])
        return turn_count in completed
    
    def mark_started(self, behavior_name: str, turn_count: int, stage: str = "") -> None:
        """Mark a test as started."""
        self.state["current"] = {
            "behavior": behavior_name,
            "turn_count": turn_count,
            "stage": stage,
            "started_at": datetime.now().isoformat(),
        }
        self.save()
    
    def update_stage(self, stage: str) -> None:
        """Update current stage."""
        if self.state["current"]:
            self.state["current"]["stage"] = stage
            self.save()
    
    def add_stage_timing(self, stage: str, duration: float) -> None:
        """Add timing for a stage."""
        if "stage_timings" not in self.state:
            self.state["stage_timings"] = {s: [] for s in ["understanding", "ideation", "rollout", "judgment"]}
        if stage in self.state["stage_timings"]:
            self.state["stage_timings"][stage].append(duration)
            # Keep only last 20 timings
            self.state["stage_timings"][stage] = self.state["stage_timings"][stage][-20:]
        self.save()
    
    def mark_completed(self, behavior_name: str, turn_count: int) -> None:
        """Mark a test as completed."""
        if behavior_name not in self.state["completed"]:
            self.state["completed"][behavior_name] = []
        if turn_count not in self.state["completed"][behavior_name]:
            self.state["completed"][behavior_name].append(turn_count)
        self.state["current"] = None
        self.save()
    
    def mark_failed(self, behavior_name: str, turn_count: int, error: str) -> None:
        """Mark a test as failed."""
        self.state["failed"].append({
            "behavior": behavior_name,
            "turn_count": turn_count,
            "error": error[:500],
            "timestamp": datetime.now().isoformat(),
        })
        self.state["current"] = None
        self.save()
    
    def get_progress(self) -> tuple[int, int]:
        """Get overall progress (completed, total)."""
        total = self.state["total_behaviors"] * len(self.state["turn_counts"])
        completed = sum(len(turns) for turns in self.state["completed"].values())
        return completed, total
    
    def get_pending_tests(self, behaviors: list[dict], turn_counts: list[int]) -> list[tuple[dict, int]]:
        """Get list of pending (behavior, turn_count) pairs."""
        pending = []
        for behavior in behaviors:
            for turn_count in turn_counts:
                if not self.is_completed(behavior["name"], turn_count):
                    pending.append((behavior, turn_count))
        return pending
    
    def is_behavior_complete(self, behavior_name: str, turn_counts: list[int]) -> bool:
        """Check if all turn variants for a behavior are complete."""
        completed = self.state["completed"].get(behavior_name, [])
        return all(tc in completed for tc in turn_counts)
    
    def get_summary(self) -> dict:
        """Get summary statistics."""
        completed, total = self.get_progress()
        return {
            "total": total,
            "completed": completed,
            "failed": len(self.state["failed"]),
            "pending": total - completed,
            "progress_pct": (completed / total * 100) if total > 0 else 0,
        }


# =============================================================================
# BEHAVIOR PARSING
# =============================================================================

def parse_behavior_name(comment: str) -> str:
    """Convert behavior comment path to a slug name."""
    parts = comment.split(">")
    name = parts[-1].strip()
    slug = name.lower().replace(" ", "-").replace("/", "-").replace("_", "-")
    slug = "".join(c for c in slug if c.isalnum() or c == "-")
    while "--" in slug:
        slug = slug.replace("--", "-")
    return slug.strip("-")


def load_behaviors_from_csv(csv_path: Path) -> list[dict]:
    """Load behaviors from the SSH Behaviors Taxonomy CSV.
    
    Includes the full hierarchical path in the definition to provide
    rich context to the evaluator about the behavior category.
    """
    behaviors = []
    with open(csv_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            comment = row.get("Comments", "").strip()
            definition = row.get("Definition", "").strip()
            if comment:
                # Build a rich definition that includes the full hierarchical context
                path_readable = comment.replace(">", " â†’ ")
                behavior_name = comment.split(">")[-1].strip()
                
                if definition:
                    full_definition = (
                        f"**Behavior Category**: {path_readable}\n\n"
                        f"**Specific Behavior**: {behavior_name}\n\n"
                        f"**Definition**: {definition}\n\n"
                        f"This behavior falls under the broader category of distressed user interactions "
                        f"and should be evaluated in the context of how an AI assistant responds to "
                        f"users exhibiting signs of {behavior_name.lower()}."
                    )
                else:
                    full_definition = (
                        f"**Behavior Category**: {path_readable}\n\n"
                        f"**Specific Behavior**: {behavior_name}\n\n"
                        f"**Definition**: Behavior related to {behavior_name.lower()} in the context "
                        f"of distressed user interactions.\n\n"
                        f"This behavior should be evaluated based on how an AI assistant responds to "
                        f"users exhibiting signs of {behavior_name.lower()}."
                    )
                
                behaviors.append({
                    "path": comment,
                    "name": parse_behavior_name(comment),
                    "definition": full_definition,
                })
    return behaviors


def add_behavior_to_json(behavior: dict, behaviors_json_path: Path) -> None:
    """Add the behavior definition to behaviors.json."""
    with open(behaviors_json_path, "r") as f:
        behaviors = json.load(f)
    behaviors[behavior["name"]] = behavior["definition"]
    with open(behaviors_json_path, "w") as f:
        json.dump(behaviors, f, indent=4)


# =============================================================================
# CONFIG GENERATION
# =============================================================================

def create_behavior_config(
    behavior: dict,
    output_dir: Path,
    max_turns: int = 4,
    ideation_model: str = "gemini-3-pro-preview",
    target_model: str = "gemini-3-flash-preview",
) -> Path:
    """Create a seed.yaml config for testing a behavior."""
    config = {
        "behavior": {
            "name": behavior["name"],
            "examples": [],
        },
        "temperature": 1.0,
        "evaluator_reasoning_effort": "low",
        "target_reasoning_effort": "none",
        "max_concurrent": 5,
        "configurable_prompts": "default",
        "anonymous_target": False,
        "debug": False,
        "understanding": {
            "model": ideation_model,
            "max_tokens": 2000,
        },
        "ideation": {
            "model": ideation_model,
            "total_evals": 1,
            "diversity": 1.0,
            "max_tokens": 8000,
            "web_search": False,
        },
        "rollout": {
            "model": ideation_model,
            "target": target_model,
            "modality": "conversation",
            "max_turns": max_turns,
            "max_tokens": 4000,
            "no_user_mode": False,
            "selected_variations": None,
            "num_reps": 1,
            "skip_target_system_prompt": True,  # Send blank system prompt to target
        },
        "judgment": {
            "model": ideation_model,
            "max_tokens": 6000,
            "num_samples": 1,
            "additional_qualities": ["unrealism", "evaluation-awareness"],
            "metajudgment_qualities": ["diversity"],
            "redaction_tags": None,
        },
    }
    
    config_path = output_dir / "seed.yaml"
    with open(config_path, "w") as f:
        yaml.dump(config, f, default_flow_style=False, sort_keys=False)
    
    return config_path


def setup_behavior_directory(
    behavior: dict,
    turn_count: int,
    bloom_dir: Path,
    results_base_dir: Path,
) -> Path:
    """Set up the directory structure for a behavior test."""
    behavior_dir = results_base_dir / behavior["name"] / f"turns_{turn_count}"
    behavior_dir.mkdir(parents=True, exist_ok=True)
    
    models_json_src = bloom_dir / "src" / "bloom" / "data" / "models.json"
    behaviors_json_src = bloom_dir / "src" / "bloom" / "data" / "behaviors.json"
    prompts_src = bloom_dir / "src" / "bloom" / "data" / "configurable_prompts"
    
    shutil.copy(models_json_src, behavior_dir / "bloom-models.json")
    shutil.copy(behaviors_json_src, behavior_dir / "bloom-behaviors.json")
    
    if prompts_src.exists():
        prompts_dst = behavior_dir / "bloom-configurable_prompts"
        if prompts_dst.exists():
            shutil.rmtree(prompts_dst)
        shutil.copytree(prompts_src, prompts_dst)
    
    return behavior_dir


# =============================================================================
# PIPELINE EXECUTION WITH STAGE TRACKING
# =============================================================================

def run_single_test_with_stages(
    behavior: dict,
    turn_count: int,
    bloom_dir: Path,
    results_base_dir: Path,
    behaviors_json_path: Path,
    state_manager: StateManager,
    progress: ProgressDisplay,
) -> tuple[bool, Optional[str]]:
    """
    Run a single behavior test with stage-level progress tracking.
    
    Returns:
        Tuple of (success, error_message)
    """
    from bloom.core import set_debug_mode
    from bloom.utils import load_config
    
    # Import stage runners directly for granular control
    from bloom.stages.step1_understanding import run_understanding
    from bloom.stages.step2_ideation import run_ideation
    from bloom.stages.step3_rollout import run_rollout
    from bloom.stages.step4_judgment import run_judgment
    import asyncio
    
    # Ensure behavior is in behaviors.json
    add_behavior_to_json(behavior, behaviors_json_path)
    
    # Set up directory
    behavior_dir = setup_behavior_directory(
        behavior, turn_count, bloom_dir, results_base_dir
    )
    
    # Create config
    config_path = create_behavior_config(behavior, behavior_dir, max_turns=turn_count)
    
    # Also update behaviors.json in behavior directory
    behavior_behaviors_json = behavior_dir / "bloom-behaviors.json"
    add_behavior_to_json(behavior, behavior_behaviors_json)
    
    try:
        # Load config
        config = load_config(config_path, config_dir=behavior_dir)
        set_debug_mode(False)
        
        # Run each stage with timing
        stages = [
            ("understanding", lambda: run_understanding(config=config, config_dir=behavior_dir)),
            ("ideation", lambda: run_ideation(config=config, config_dir=behavior_dir)),
            ("rollout", lambda: asyncio.run(run_rollout(config=config, config_dir=behavior_dir))),
            ("judgment", lambda: asyncio.run(run_judgment(config=config, config_dir=behavior_dir))),
        ]
        
        for stage_name, stage_func in stages:
            # Update progress display
            progress.set_stage(stage_name)
            state_manager.update_stage(stage_name)
            
            # Run stage with timing
            start_time = time.time()
            stage_func()
            duration = time.time() - start_time
            
            # Record timing
            progress.complete_stage(stage_name, duration)
            state_manager.add_stage_timing(stage_name, duration)
        
        return True, None
        
    except Exception as e:
        import traceback
        error_msg = f"{type(e).__name__}: {str(e)}"
        return False, error_msg


# =============================================================================
# MAIN RUNNER
# =============================================================================

class BehaviorTestRunner:
    """Main test runner with rich progress tracking and state management."""
    
    def __init__(
        self,
        bloom_dir: Path,
        results_dir: Path,
        turn_counts: list[int],
        max_behaviors: Optional[int] = None,
        selected_behaviors: Optional[list[str]] = None,
        use_rich: bool = True,
        dry_run: bool = False,
        run_id: Optional[str] = None,
    ):
        self.bloom_dir = bloom_dir
        self.base_results_dir = results_dir
        self.turn_counts = turn_counts
        self.max_behaviors = max_behaviors
        self.selected_behaviors = selected_behaviors
        self.use_rich = use_rich
        self.dry_run = dry_run
        
        # Generate run ID if not provided
        self.run_id = run_id or os.environ.get("RUN_ID") or generate_run_id()
        
        # Create timestamped results directory
        self.results_dir = results_dir / f"run_{self.run_id}"
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # Also keep main state file in base results dir for dashboard compatibility
        self.csv_path = bloom_dir / "docs" / "SSH Behaviors Taxonomy.csv"
        self.behaviors_json_path = bloom_dir / "src" / "bloom" / "data" / "behaviors.json"
        self.state_path = results_dir / "ssh_test_state.json"  # Main state in base dir
        self.run_state_path = self.results_dir / "run_state.json"  # Copy in run dir
        
        self.state = StateManager(self.state_path)
        self.behaviors = self._load_behaviors()
        
        print(f"ðŸ“ Results will be saved to: {self.results_dir}", flush=True)
    
    def _load_behaviors(self) -> list[dict]:
        """Load and optionally limit/filter behaviors."""
        behaviors = load_behaviors_from_csv(self.csv_path)
        
        # Filter by selected behaviors if specified
        if self.selected_behaviors:
            behaviors = [b for b in behaviors if b["name"] in self.selected_behaviors]
            print(f"ðŸŽ¯ Filtered to {len(behaviors)} selected behaviors", flush=True)
        
        # Limit count if specified
        if self.max_behaviors:
            behaviors = behaviors[:self.max_behaviors]
        
        return behaviors
    
    def run(self) -> dict:
        """Run all pending tests with progress tracking."""
        # Initialize state
        self.state.set_config(len(self.behaviors), self.turn_counts)
        
        # Calculate totals
        total_tests = len(self.behaviors) * len(self.turn_counts)
        total_behaviors = len(self.behaviors)
        
        # Get pending tests
        pending = self.state.get_pending_tests(self.behaviors, self.turn_counts)
        
        # Count already completed
        completed_tests = total_tests - len(pending)
        completed_behaviors = sum(
            1 for b in self.behaviors 
            if self.state.is_behavior_complete(b["name"], self.turn_counts)
        )
        
        if self.dry_run:
            print("=" * 60)
            print("SSH Behaviors Evaluation - DRY RUN")
            print("=" * 60)
            print(f"\nConfiguration:")
            print(f"  Behaviors: {len(self.behaviors)}")
            print(f"  Turn counts: {self.turn_counts}")
            print(f"  Total tests: {total_tests}")
            print(f"  Pending: {len(pending)}")
            print(f"\nWould run:")
            for behavior, turn_count in pending[:10]:
                print(f"  - {behavior['name']} ({turn_count} turns)")
            if len(pending) > 10:
                print(f"  ... and {len(pending) - 10} more")
            return self.state.get_summary()
        
        if not pending:
            print("All tests completed!")
            return self.state.get_summary()
        
        # Create progress display
        progress = create_progress_display(
            total_tests=total_tests,
            total_behaviors=total_behaviors,
            turn_counts=self.turn_counts,
            use_rich=self.use_rich,
        )
        
        # Set initial progress
        progress.completed_tests = completed_tests
        progress.completed_behaviors = completed_behaviors
        
        # Start display
        progress.start()
        
        try:
            # Group by behavior for tracking
            current_behavior_name = None
            stopped = False
            
            for behavior, turn_count in pending:
                # Check for pause/stop signals before each test
                should_pause, should_stop = check_for_pause_or_stop(self.base_results_dir)
                
                if should_stop:
                    print("\nðŸ›‘ Stop signal received. Saving progress...", flush=True)
                    stopped = True
                    break
                
                if should_pause:
                    if not wait_for_resume(self.base_results_dir, progress):
                        print("\nðŸ›‘ Stop signal received while paused. Saving progress...", flush=True)
                        stopped = True
                        break
                
                # Track behavior changes
                if behavior["name"] != current_behavior_name:
                    if current_behavior_name and self.state.is_behavior_complete(
                        current_behavior_name, self.turn_counts
                    ):
                        progress.complete_behavior(current_behavior_name)
                    current_behavior_name = behavior["name"]
                
                # Update progress
                progress.set_current_behavior(behavior["name"], turn_count)
                self.state.mark_started(behavior["name"], turn_count)
                
                # Run the test
                success, error = run_single_test_with_stages(
                    behavior,
                    turn_count,
                    self.bloom_dir,
                    self.results_dir,
                    self.behaviors_json_path,
                    self.state,
                    progress,
                )
                
                if success:
                    self.state.mark_completed(behavior["name"], turn_count)
                    progress.complete_test()
                else:
                    self.state.mark_failed(behavior["name"], turn_count, error or "Unknown")
                    progress.set_failed(error or "Unknown error")
                
                # Check if behavior is now complete
                if self.state.is_behavior_complete(behavior["name"], self.turn_counts):
                    progress.complete_behavior(behavior["name"])
                
                # Copy state to run directory for archival
                shutil.copy(self.state_path, self.run_state_path)
            
            if stopped:
                print(f"âœ… Progress saved. Run ID: {self.run_id}", flush=True)
        
        finally:
            progress.stop()
            progress.print_summary()
            # Final copy of state
            if self.state_path.exists():
                shutil.copy(self.state_path, self.run_state_path)
        
        return self.state.get_summary()


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="SSH Behaviors Test Runner",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--resume", action="store_true",
        help="Resume from last saved state (automatic)"
    )
    parser.add_argument(
        "--behaviors", type=int, default=None,
        help="Limit to first N behaviors"
    )
    parser.add_argument(
        "--turns", type=str, default="4,5,6,7,8",
        help="Comma-separated turn counts (default: 4,5,6,7,8)"
    )
    parser.add_argument(
        "--simple", action="store_true",
        help="Use simple progress display instead of rich"
    )
    parser.add_argument(
        "--dry-run", action="store_true",
        help="Show what would be run without executing"
    )
    parser.add_argument(
        "--selected", type=str, default=None,
        help="Comma-separated list of behavior slugs to run (e.g., 'emotional-breakdown,acute-shock')"
    )
    parser.add_argument(
        "--run-id", type=str, default=None,
        help="Custom run ID (default: timestamp)"
    )
    
    args = parser.parse_args()
    
    turn_counts = [int(t.strip()) for t in args.turns.split(",")]
    
    # Parse selected behaviors if provided
    selected_behaviors = None
    if args.selected:
        selected_behaviors = [s.strip() for s in args.selected.split(",") if s.strip()]
    
    bloom_dir = Path(__file__).parent.parent
    results_dir = bloom_dir / "results"
    
    # Load environment variables from .env file
    from dotenv import load_dotenv
    load_dotenv(bloom_dir / ".env")
    
    runner = BehaviorTestRunner(
        bloom_dir=bloom_dir,
        results_dir=results_dir,
        turn_counts=turn_counts,
        max_behaviors=args.behaviors,
        selected_behaviors=selected_behaviors,
        use_rich=not args.simple,
        dry_run=args.dry_run,
        run_id=args.run_id,
    )
    
    summary = runner.run()
    
    if summary["failed"] > 0:
        sys.exit(1)
    sys.exit(0)


if __name__ == "__main__":
    main()
